{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料來源\n",
    "##### https://openweathermap.org/forecast5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:18:36.274585Z",
     "start_time": "2021-05-27T13:18:36.008846Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存在 Windows 和 Ubuntu 的檔案名稱不同，在計算下載時間的時候會錯誤\n",
    "# 透過程式將 2 種命名格式調整至固定格式\n",
    "def replacer(s, newstring, index, nofail=False):\n",
    "    # raise an error if index is outside of the string\n",
    "    if not nofail and index not in range(len(s)):\n",
    "        raise ValueError(\"index outside given string\")\n",
    "\n",
    "    # if not erroring, but the index is still not in the correct range..\n",
    "    if index < 0:  # add it to the beginning\n",
    "        return newstring + s\n",
    "    if index > len(s):  # add it to the end\n",
    "        return s + newstring\n",
    "\n",
    "    # insert the new string between \"slices\" of the original\n",
    "    return s[:index] + newstring + s[index + 1:]\n",
    "def rebuild_crawler_time(string):\n",
    "    try:\n",
    "        new_string = string.replace(\".txt\", \"\")\n",
    "        new_string = string.replace(\".csv\", \"\")\n",
    "        new_string = new_string.replace('%3A', ':')\n",
    "        new_string = new_string.replace('_', ':')\n",
    "        new_string = new_string[len(new_string)-19:len(new_string)-0]\n",
    "#         print(new_string)\n",
    "        if(new_string.count(':')>2):\n",
    "            new_string = replacer(new_string, \" \", new_string.find(':'))\n",
    "        new_string = pd.to_datetime(new_string)\n",
    "#         print(new_string)\n",
    "    except:\n",
    "        new_string = ''\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打包 3 小時預報資料\n",
    "### 請先備份並下載原始預報資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def aggregate_data(target_date, directory='.\\\\OpenWeatherMap.3H'):\n",
    "#     array = []\n",
    "\n",
    "#     #讀取資料夾\n",
    "#     for folder in os.listdir(directory):\n",
    "#     #     print(folder)\n",
    "#         #跳過\n",
    "#         if(folder=='Save'):\n",
    "#             continue\n",
    "\n",
    "#         #讀取csv \n",
    "#         for filename in glob.glob(f'{directory}\\\\{folder}\\\\*.csv'):\n",
    "#             filetime = rebuild_crawler_time(filename.split('\\\\')[3])\n",
    "#             start, end = target_date, target_date+datetime.timedelta(days=1)\n",
    "#             if end>filetime>start:\n",
    "#                 try:\n",
    "# #                     print(filetime)\n",
    "#                     temp = pd.read_csv(filename)\n",
    "#                     temp['CrawlerTime'] = filetime\n",
    "#                     array.append(temp)\n",
    "#                 except:\n",
    "#             #             print(filename)\n",
    "#                     pass\n",
    "\n",
    "#     package = pd.concat(array, axis=0, ignore_index=True)\n",
    "#     package.to_csv(f'{directory}/Save/OWM.3H.Raw(new).csv', index=None)\n",
    "#     return package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data2(start_date, end_date, directory='.\\\\OpenWeatherMap.3H'):\n",
    "    array = []\n",
    "\n",
    "    #讀取資料夾\n",
    "    for folder in os.listdir(directory):\n",
    "    #     print(folder)\n",
    "        #跳過\n",
    "        if(folder=='Save'):\n",
    "            continue\n",
    "\n",
    "        #讀取csv \n",
    "        for filename in glob.glob(f'{directory}\\\\{folder}\\\\*.csv'):\n",
    "            filetime = rebuild_crawler_time(filename.split('\\\\')[3])\n",
    "#             start, end = target_date, target_date+datetime.timedelta(days=1)\n",
    "            if end_date>filetime>start_date:\n",
    "                try:\n",
    "#                     print(filetime)\n",
    "                    temp = pd.read_csv(filename)\n",
    "                    temp['CrawlerTime'] = filetime\n",
    "                    array.append(temp)\n",
    "                except:\n",
    "            #             print(filename)\n",
    "                    pass\n",
    "\n",
    "    package = pd.concat(array, axis=0, ignore_index=True)\n",
    "    package.to_csv(f'{directory}/Save/OWM.3H.Raw(new).csv', index=None)\n",
    "    return package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T11:46:36.792522Z",
     "start_time": "2021-05-19T11:45:32.647662Z"
    }
   },
   "outputs": [],
   "source": [
    "# tdate = pd.to_datetime('2022-04-18')\n",
    "# forecast = aggregate_data(tdate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 整理 3 小時預報資料\n",
    "### 請先將個別的原始檔案彙整成 1 個檔案，再執行本程式\n",
    "### 包含時間轉換、重新命名、去除重覆資料.....等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T14:32:38.617410Z",
     "start_time": "2021-05-20T14:32:38.586478Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def unix_to_datetime(unix):\n",
    "    unix = int(unix)\n",
    "    return datetime.utcfromtimestamp(unix).strftime('%Y-%m-%d %H:%M:%S')\n",
    "def uct_to_local_time(utc_time):\n",
    "    os.environ['TZ']='Asia/Taipei'\n",
    "    d = datetime.strptime(utc_time,\"%Y-%m-%d %H:%M:%S\")\n",
    "    utc_ts = calendar.timegm(d.utctimetuple())\n",
    "    return datetime.fromtimestamp(utc_ts).replace(microsecond=d.microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T14:32:38.818805Z",
     "start_time": "2021-05-20T14:32:38.807951Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def rebuild_crawler_time(string):\n",
    "#     new_string = string.replace(\".csv\", \"\")\n",
    "#     new_string = new_string[len(new_string)-26:len(new_string)-7].replace(\"_\", \" \")\n",
    "#     t1, t2 = new_string[:10], new_string[10:].replace('-', ':')\n",
    "#     try:\n",
    "#         new_string = pd.to_datetime(t1+t2)\n",
    "#     except:\n",
    "#         new_string = ''\n",
    "#     return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T15:21:13.468964Z",
     "start_time": "2021-05-20T15:21:00.335138Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sort_data_3h():\n",
    "    # 讀取原始預報資料：剛從原始檔案彙整起來，尚未處理過的檔案\n",
    "    file = 'OpenWeatherMap.3H/Save/OWM.3H.Raw(new).csv'\n",
    "    package = pd.read_csv(file, low_memory=False)\n",
    "\n",
    "    # 原始檔案的變數名稱有很多縮寫，為避免未來看不懂，故更改成較完整的名稱\n",
    "    # num = 預測天數的序號(可以拿來排序預測週期的長短)\n",
    "    package = package.rename(columns={\n",
    "        'id': 'ID', 'name': 'Name',\n",
    "        'dt': 'TIME_TO_INTERVAL',\n",
    "        'lat': 'Latitude', 'lon': 'Longitude',\n",
    "        'feels_like': 'FeelsLikeTemperature(pred)',\n",
    "        'temp': 'Temperature(pred)',\n",
    "        'temp_min': 'MaxTemperature(pred)',\n",
    "        'temp_max': 'MinTemperature(pred)',\n",
    "        'pop': 'PoP(pred)', \n",
    "        'rain': 'Rain(pred)',\n",
    "        'wind_speed': 'WindSpeed(pred)',\n",
    "        'wind_deg': 'WindDirection(pred)',\n",
    "        'humidity': 'RelativeHumidity(pred)',\n",
    "        'pressure': 'Pressure(pred)',\n",
    "        'sea_level': 'AtmosphericPressureSea(pred)',\n",
    "        'grnd_level': 'AtmosphericPressureGround(pred)',\n",
    "        'visibility': 'Visibility(pred)',\n",
    "        'weather_description': 'WeatherType',\n",
    "        'weather_main': 'WeatherType(main)',\n",
    "        'weather_id': 'WeatherType(index)'\n",
    "    }, inplace=False)\n",
    "    package = package.drop(['Unnamed: 0', 'temp_kf', 'snow', 'dt_txt'], axis=1, errors='ignore')\n",
    "\n",
    "    # 透過 \"uct_to_local_time()\" 轉換成相同時區\n",
    "    package['TIME_TO_INTERVAL'] = package['TIME_TO_INTERVAL'].apply(lambda x: uct_to_local_time(unix_to_datetime(x)))\n",
    "    package['TIME_TO_INTERVAL'] = pd.to_datetime(package['TIME_TO_INTERVAL'])\n",
    "    package['CrawlerTime'] = pd.to_datetime(package['CrawlerTime'])\n",
    "    package = package.sort_values(by=['ID', 'TIME_TO_INTERVAL', 'CrawlerTime'], inplace=False)\n",
    "    package = package.reset_index(inplace=False, drop=True)\n",
    "\n",
    "    # Convert temperature unit\n",
    "    package['Temperature(pred)'] = (package['Temperature(pred)']-273.15)\n",
    "    package['MaxTemperature(pred)'] = (package['MaxTemperature(pred)']-273.15)\n",
    "    package['MinTemperature(pred)'] = (package['MinTemperature(pred)']-273.15)\n",
    "    package['FeelsLikeTemperature(pred)'] = (package['FeelsLikeTemperature(pred)']-273.15)\n",
    "\n",
    "    # 早期有根據天泰案場的經緯度，抓幾個特定案場的預報，但是後來取消了\n",
    "    # 為了資料的一致性，要過濾掉這些案場早期蒐集的預報資料\n",
    "    package = package[~package['ID'].isnull()]\n",
    "    package = package[~package['ID'].isin(['TCG01', 'THCY01', 'TJI01', 'TZQ01'])]\n",
    "\n",
    "    # 移除重覆的資料(1)： \"TIME_TO_INTERVAL\"(資料時間) & \"CrawlerTime\" 相同(預測時間)\n",
    "    drop = package.copy()\n",
    "    drop = drop.sort_values(by=['ID', 'TIME_TO_INTERVAL', 'CrawlerTime'])\n",
    "    drop = drop.drop_duplicates(['ID', 'TIME_TO_INTERVAL', 'CrawlerTime'], keep=\"last\")\n",
    "    drop = drop.reset_index(inplace=False, drop=True)\n",
    "    print(len(package)-len(drop))\n",
    "\n",
    "    file = 'OpenWeatherMap.3H/Save/OWM.3H.Merge.Raw(new).csv'\n",
    "    drop.to_csv(file, index=False)\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop = sort_data_3h()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 整合新舊資料 & 填充缺值\n",
    "### 將本次整理的 3 小時預報和先前的合併\n",
    "### 同時，根據預測時間，將資料分為 24 小時內和 24 小時前的預報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T15:31:10.671787Z",
     "start_time": "2021-05-20T15:31:10.657072Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 根據提前預報的時間，將資料分成:「 24小時前 」,「 24小時內」\n",
    "def build_multiple_lead_time_data(data):\n",
    "    forecast = data.copy()\n",
    "    forecast['TIME_TO_INTERVAL'] = pd.to_datetime(forecast['TIME_TO_INTERVAL'])\n",
    "    forecast['CrawlerTime'] = pd.to_datetime(forecast['CrawlerTime'])\n",
    "    forecast['TimeAhead'] = forecast['TIME_TO_INTERVAL'] - forecast['CrawlerTime']\n",
    "    forecast['DayAhead'] = forecast['TIME_TO_INTERVAL'].dt.date - forecast['CrawlerTime'].dt.date\n",
    "    forecast.sort_values(by=['ID', 'TIME_TO_INTERVAL', 'TimeAhead'], inplace=True)\n",
    "    forecast = forecast.reset_index(inplace=False, drop=True)\n",
    "    \n",
    "    # 01. 24 小時內\n",
    "    merge0 = forecast.copy()\n",
    "    merge0 = merge0.drop_duplicates(['ID', 'TIME_TO_INTERVAL'], keep=\"first\")  \n",
    "    merge0['TimeAhead'] = 0\n",
    "    print('merge0' ,len(merge0))\n",
    "\n",
    "    # 02. 24 小時前\n",
    "    merge24 = forecast.copy()\n",
    "    merge24 = merge24[merge24['DayAhead']>=timedelta(days=1)]\n",
    "    merge24 = merge24.drop_duplicates(['ID', 'TIME_TO_INTERVAL'], keep=\"first\")  \n",
    "    merge24['TimeAhead'] = 24\n",
    "    print('merge24', len(merge24))\n",
    "    \n",
    "    build = pd.concat([merge24, merge0], axis=0, ignore_index=True)\n",
    "    build = build.drop(['DayAhead'], axis=1)\n",
    "    return build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T15:31:12.123072Z",
     "start_time": "2021-05-20T15:31:12.107444Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 使用「 24小時內 」的預測資料，填補「 24小時前 」資料的缺值\n",
    "def fill_multiple_lead_time_data(data):\n",
    "    forecast = data.copy()\n",
    "    forecast['TIME_TO_INTERVAL'] = pd.to_datetime(forecast['TIME_TO_INTERVAL'])\n",
    "    forecast['CrawlerTime'] = pd.to_datetime(forecast['CrawlerTime'])\n",
    "    forecast = forecast.sort_values(by=['ID', 'TIME_TO_INTERVAL', 'CrawlerTime'], inplace=False)\n",
    "    forecast = forecast.reset_index(inplace=False, drop=True)\n",
    "    \n",
    "    # 01.\n",
    "    merge0 = forecast.copy()\n",
    "    merge0 = merge0.drop_duplicates(['ID', 'TIME_TO_INTERVAL'], keep=\"last\")  \n",
    "    merge0['TimeAhead'] = 0\n",
    "    print('merge0' ,len(merge0))\n",
    "    \n",
    "    # 02.\n",
    "    merge24 = forecast.copy()\n",
    "    merge24 = merge24[merge24['TimeAhead'].eq(24)]\n",
    "    filling = merge0[~merge0['TIME_TO_INTERVAL'].isin(merge24['TIME_TO_INTERVAL'].tolist())]\n",
    "    merge24 = pd.concat([merge24, filling]).reset_index(inplace=False, drop=True)\n",
    "    merge24['TimeAhead'] = 24\n",
    "    print('merge24', len(merge24))\n",
    "    \n",
    "    build = pd.concat([merge24, merge0], axis=0, ignore_index=True)\n",
    "    build = build.drop_duplicates(['ID', 'TIME_TO_INTERVAL', 'TimeAhead'], keep=\"last\")\n",
    "    return build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T15:31:32.319799Z",
     "start_time": "2021-05-20T15:31:12.543213Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def merge_data_24(drop):\n",
    "    # 標記「 24小時內 」和「 24小時前 」的預報\n",
    "    forecast = drop.copy()\n",
    "    forecast = build_multiple_lead_time_data(forecast)\n",
    "\n",
    "    # 整合新資料與舊資料\n",
    "    history = pd.read_csv(f'OpenWeatherMap.3H/Save/OWM.3H.Merge.Multiple(merge).csv')\n",
    "    history['TIME_TO_INTERVAL'] = pd.to_datetime(history['TIME_TO_INTERVAL'])\n",
    "    merge = pd.concat([forecast, history], axis=0, ignore_index=True)\n",
    "    # merge = forecast.copy()\n",
    "    merge['TIME_TO_INTERVAL'] = pd.to_datetime(merge['TIME_TO_INTERVAL'])\n",
    "    merge['CrawlerTime'] = pd.to_datetime(merge['CrawlerTime'])\n",
    "    merge = merge.sort_values(by=['ID', 'TIME_TO_INTERVAL', 'CrawlerTime'], inplace=False)\n",
    "    merge = merge.drop_duplicates(['TIME_TO_INTERVAL', 'ID', 'TimeAhead'], keep=\"last\")\n",
    "\n",
    "    # 標記「 24小時內 」和「 24小時前 」的預報\n",
    "    merge = fill_multiple_lead_time_data(merge)\n",
    "    print(f'history: {len(forecast)}, merge: {len(merge)}')\n",
    "\n",
    "    file = 'OpenWeatherMap.3H/Save/OWM.3H.Merge.Multiple(new).csv'\n",
    "    forecast.to_csv(file, index=False)\n",
    "    file = 'OpenWeatherMap.3H/Save/OWM.3H.Merge.Multiple(merge).csv'\n",
    "    merge.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "merge0 1299\n",
      "merge24 1296\n",
      "merge0 7709\n",
      "merge24 7709\n",
      "history: 2595, merge: 15418\n"
     ]
    }
   ],
   "source": [
    "# start,end = pd.to_datetime('2022-08-12'),pd.to_datetime('2023-01-19')\n",
    "# forecast = aggregate_data2(start,end)\n",
    "# drop = sort_data_3h()\n",
    "# merge_data_24(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_data_24(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date, end_date = pd.to_datetime('2022-04-20'), pd.to_datetime('2022-04-21')\n",
    "# forecast = aggregate_data2(start_date,end_date)\n",
    "# drop = sort_data_3h()\n",
    "# merge_data_24(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('./OpenWeatherMap.3H/Save/OWM.3H.Merge.Multiple(merge)2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
